{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import ast\n",
    "import codecs\n",
    "import json\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import random\n",
    "import fasttext\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from ruwordnet import RuWordNet\n",
    "\n",
    "FORMAT = \"%(asctime)s %(levelname)s: %(message)s\"\n",
    "logging.basicConfig(level=logging.DEBUG,format=FORMAT)\n",
    "\n",
    "from utils import _join, PUNCT_SYMBOLS, PREPOSITIONS, RESTRICT_TAGS, \\\n",
    "                  get_synset_str_and_vector_lemma, get_synset_words_lemma, \\\n",
    "                  get_top, get_top_hyperomyns_counter, get_top_hyperomyns_counter_v, \\\n",
    "                  read_train_dataset, read_train_dataset, read_gold_dataset, \\\n",
    "                  normalize_ma_lemmatize, \\\n",
    "                  save_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph_analyzer = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prefix = 'lemm_lower_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/res_'+common_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {results_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_grid():\n",
    "    norm_functions = [normalize_ma_lemmatize]\n",
    "    norm_fn_out_of_vocab2synonym = [None]\n",
    "    norm_fn_sort   = [False]\n",
    "    norm_fn_unique = [False] \n",
    "    norm_fn_lower  = [True]\n",
    "    norm_fn_min_word_len = [1]\n",
    "\n",
    "    punct_symbols = set(PUNCT_SYMBOLS).copy()\n",
    "    punct_symbols_ = punct_symbols\n",
    "    \n",
    "    # special symbols from ruwordnet\n",
    "    from_ruwordnet_punct_symbols = {'—', '«', '»', '·', '\\xad', '\\xa0', '°', '–', '§'} \n",
    "    \n",
    "    punct_symbols.update(from_ruwordnet_punct_symbols)\n",
    "    \n",
    "    norm_fn_punct_symbols = [punct_symbols]\n",
    "    norm_fn_prepositions  = [PREPOSITIONS]\n",
    "    norm_fn_restrict_tags = [RESTRICT_TAGS]\n",
    "\n",
    "    #verb_tags = ['VERB', 'INFN', 'GRND', 'NOUN']\n",
    "    norm_fn_accept_tags = [None]\n",
    "\n",
    "#     p1_l = [-1.0, 0.0, 0.1, 0.5, 1.0, 2.0]\n",
    "#     p1_l = [0.0, 0.1, 1.0]\n",
    "    p1_l = [0.1]\n",
    "#     p2_l = [-1.0, 0.0, 0.1, 0.5, 1.0, 2.0]\n",
    "#     p2_l = [0.0, 0.1, 1.0]\n",
    "    p2_l = [1.0]\n",
    "#     p3_l = [-1.0, 0.0, 0.1, 0.5, 1.0, 2.0]\n",
    "#     p3_l = [0.0, 0.1, 1.0]\n",
    "    p3_l = [1.0]\n",
    "#     k_l  = [1, 3, 5, 7, 10, 15, 20]\n",
    "    k_l  = [10]\n",
    "#     topn_l = [1, 3, 5, 7, 10, 15, 20]\n",
    "    topn_l  = [10]\n",
    "    account_gold_l = [False]\n",
    "    \n",
    "    all_params = {'p1': p1_l,\n",
    "                  'p2': p2_l,\n",
    "                  'p3': p3_l,\n",
    "                  'k' : k_l, \n",
    "                  'topn': topn_l,\n",
    "                  'account_gold': account_gold_l,\n",
    "                  \n",
    "                  'normalize_func': norm_functions,\n",
    "                  \n",
    "                  'out_of_vocab2synonym': norm_fn_out_of_vocab2synonym,\n",
    "                  'sort': norm_fn_sort,\n",
    "                  'unique': norm_fn_unique,\n",
    "                  'lower': norm_fn_lower,\n",
    "                  'min_word_len': norm_fn_min_word_len,\n",
    "                  'punct_symbols': norm_fn_punct_symbols,\n",
    "                  'prepositions': norm_fn_prepositions,\n",
    "                  'restrict_tags': norm_fn_restrict_tags,\n",
    "                  'accept_tags': norm_fn_accept_tags,\n",
    "                  'ma': [morph_analyzer]\n",
    "                 }\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in ParameterGrid(all_params):\n",
    "        yield p\n",
    "        \n",
    "len([params for params in tqdm(params_grid())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = list(params_grid())[0]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ruwordnet = RuWordNet('data/ruwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ft_model_file='fasttext/cc.ru.300.bin'\n",
    "ft_model = fasttext.load_model(ft_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tayga_none_fasttextcbow_300_10_2019='data/rusvectores_models/tayga_none_fasttextcbow_300_10_2019/model.model'\n",
    "model_tayga = KeyedVectors.load(tayga_none_fasttextcbow_300_10_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "araneum_none_fasttextcbow_300_5_2018 = 'data/rusvectores_models/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model_araneum = KeyedVectors.load(araneum_none_fasttextcbow_300_5_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname2model = {'ft.cc.ru.300': ft_model,\n",
    "                   'tayga_none_fasttextcbow_300_10_2019': model_tayga,\n",
    "                   'araneum_none_fasttextcbow_300_5_2018': model_araneum,\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words, vector = get_synset_str_and_vector_lemma(ruwordnet, \n",
    "                                          '116365-V', \n",
    "                                          dict(), \n",
    "                                          {'ruthes_name': True, \n",
    "                                           'senses_names': True}, \n",
    "                                          norm_function=normalize_ma_lemmatize, \n",
    "                                          model=ft_model)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words, vector = get_synset_str_and_vector_lemma(ruwordnet, \n",
    "                                                '9577-N', \n",
    "                                                dict(), \n",
    "                                              {'ruthes_name': True, \n",
    "                                               'senses_names': True}, \n",
    "                                                norm_function=normalize_ma_lemmatize, \n",
    "                                                model=model_tayga)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nouns=True  # NOUNS\n",
    "nouns=False # VERBS\n",
    "\n",
    "p1,p2,p3,k,topn,account_gold,norm_function = params['p1'],params['p2'],params['p3'],\\\n",
    "                                             params['k'],params['topn'],params['account_gold'],\\\n",
    "                                             params['normalize_func']\n",
    "\n",
    "pos = \"N\" if nouns else \"V\"\n",
    "\n",
    "norm_fn_params_list = {'out_of_vocab2synonym',\n",
    "                       'sort',\n",
    "                       'unique',\n",
    "                       'lower',\n",
    "                       'min_word_len',\n",
    "                       'punct_symbols',\n",
    "                       'prepositions',\n",
    "                       'restrict_tags',\n",
    "                       'accept_tags',\n",
    "                       'ma'\n",
    "                       }\n",
    "norm_fn_params = {param_name: params[param_name] \n",
    "                  for param_name in norm_fn_params_list\n",
    "                  if param_name in params}\n",
    "if nouns:\n",
    "    make_sentences_params = {'ruthes_name': True,\n",
    "                             'definition': False,\n",
    "                             'senses_names': True,\n",
    "                             'senses_lemmas': False,\n",
    "                             'senses_main_word': False,\n",
    "                             'sep': ' '\n",
    "                            }\n",
    "else:\n",
    "    make_sentences_params = {'ruthes_name': True,\n",
    "                             'definition': True,\n",
    "                             'senses_names': True,\n",
    "                             'senses_lemmas': True,\n",
    "                             'senses_main_word': True,\n",
    "                             'sep': ' '\n",
    "                            }\n",
    "\n",
    "synsetstr2id = dict()\n",
    "synsetstr2vector = dict()\n",
    "for synset in ruwordnet.synsets_list:\n",
    "    if synset['part_of_speech'] != pos:\n",
    "        continue\n",
    "\n",
    "    synset_words, vector = get_synset_str_and_vector_lemma(ruwordnet, synset['id'], \n",
    "                                                     norm_params=norm_fn_params,\n",
    "                                                     make_sent_params=make_sentences_params,\n",
    "                                                     norm_function=norm_function,\n",
    "                                                     model=ft_model)\n",
    "    synsetstr = ' '.join(synset_words)\n",
    "    if synsetstr in synsetstr2id:\n",
    "        logging.error(f\"Duplicate synset_str.{synset['id'],synsetstr2id[synsetstr]}:'{synsetstr}'\")\n",
    "    synsetstr2id[synsetstr] = synset['id']\n",
    "    synsetstr2vector[synsetstr] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsetstr2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_sentences_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_synset_words_lemma(ruwordnet, '124987-N',\n",
    "                       norm_params=norm_fn_params,\n",
    "                       make_sent_params=make_sentences_params,\n",
    "                       norm_function=normalize_ma_lemmatize,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS WORDS FOR TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(words, \n",
    "                  params_grid,\n",
    "                  prefix, \n",
    "                  nouns, \n",
    "                  model,\n",
    "                  algo='default',\n",
    "                  out_dir='/tmp'):\n",
    "    for params in tqdm(params_grid()):\n",
    "        start = time.time()\n",
    "        p1,p2,p3,k,topn,account_gold,norm_function = params['p1'],params['p2'],params['p3'],\\\n",
    "                                                     params['k'],params['topn'],params['account_gold'],\\\n",
    "                                                     params['normalize_func']\n",
    "        \n",
    "        pos = \"N\" if nouns else \"V\"\n",
    "        \n",
    "        norm_fn_params_list = {'out_of_vocab2synonym',\n",
    "                               'sort',\n",
    "                               'unique',\n",
    "                               'lower',\n",
    "                               'min_word_len',\n",
    "                               'punct_symbols',\n",
    "                               'prepositions',\n",
    "                               'restrict_tags',\n",
    "                               'accept_tags',\n",
    "                               'ma'\n",
    "                               }\n",
    "        norm_fn_params = {param_name: params[param_name] \n",
    "                          for param_name in norm_fn_params_list\n",
    "                          if param_name in params}\n",
    "        if nouns:\n",
    "            make_sentences_params = {'ruthes_name': True,\n",
    "                                     'definition': False,\n",
    "                                     'senses_names': True,\n",
    "                                     'senses_lemmas': False,\n",
    "                                     'senses_main_word': False,\n",
    "                                     'sep': ' '\n",
    "                                    }\n",
    "        else:\n",
    "            make_sentences_params = {'ruthes_name': True,\n",
    "                                     'definition': True,\n",
    "                                     'senses_names': True,\n",
    "                                     'senses_lemmas': True,\n",
    "                                     'senses_main_word': True,\n",
    "                                     'sep': ' '\n",
    "                                    }\n",
    "        \n",
    "        if nouns:\n",
    "            word2parents = read_train_dataset('data/training_data/synsets_nouns.tsv', ruwordnet)\n",
    "            gold_synsetid2parents = read_gold_dataset('data/training_data/synsets_nouns.tsv')\n",
    "        else:\n",
    "            word2parents = read_train_dataset('data/training_data/synsets_verbs.tsv', ruwordnet)\n",
    "            gold_synsetid2parents = read_gold_dataset('data/training_data/synsets_verbs.tsv')\n",
    "        \n",
    "        synsetstr2id = dict()\n",
    "        synsetstr2vector = dict()\n",
    "        for synset in ruwordnet.synsets_list:\n",
    "            if synset['part_of_speech'] != pos:\n",
    "                continue\n",
    "\n",
    "            synset_words, vector = get_synset_str_and_vector_lemma(ruwordnet, synset['id'], \n",
    "                                                                   norm_params=norm_fn_params,\n",
    "                                                                   make_sent_params=make_sentences_params,\n",
    "                                                                   norm_function=norm_function,\n",
    "                                                                   model=model)\n",
    "            synsetstr = ' '.join(synset_words)\n",
    "            if synsetstr in synsetstr2id:\n",
    "                logging.error(f\"Duplicate synset_str.{synset['id'],synsetstr2id[synsetstr]}:'{synsetstr}'\")\n",
    "            synsetstr2id[synsetstr] = synset['id']\n",
    "            synsetstr2vector[synsetstr] = vector\n",
    "\n",
    "#         print (len(synsetstr2vector))\n",
    "                              \n",
    "        example_vector = None\n",
    "        for _,v in synsetstr2vector.items():\n",
    "            example_vector = v\n",
    "            break\n",
    "\n",
    "        ruwordnet_matrix = np.zeros((len(synsetstr2vector),example_vector.shape[0]),\n",
    "                                     example_vector.dtype)\n",
    "\n",
    "        for i, (s, v) in enumerate(synsetstr2vector.items()):\n",
    "            ruwordnet_matrix[i] = v\n",
    "#         print (ruwordnet_matrix.shape)\n",
    "                \n",
    "        \n",
    "        result = collections.defaultdict(list)\n",
    "        for w in words:\n",
    "            hypernyms = get_top_hyperomyns_counter(w, \n",
    "                                                   k=k, \n",
    "                                                   p1=p1,p2=p2,p3=p3, \n",
    "                                                   account_gold=account_gold,\n",
    "                                                   ruwordnet_matrix=ruwordnet_matrix,\n",
    "                                                   gold_synsetid2parents=gold_synsetid2parents,\n",
    "                                                   synsetstr2id=synsetstr2id,\n",
    "                                                   synsetstr2vector=synsetstr2vector,\n",
    "                                                   model=model,\n",
    "                                                   ruwordnet=ruwordnet\n",
    "                                                  )\n",
    "            result[w] = [h for h,rate in hypernyms.most_common()][:topn]\n",
    "            \n",
    "        \n",
    "        curr_time ='_'.join(str(datetime.now()).split()).replace(':','')\n",
    "        out_file  = f'{prefix}_{k}_{topn}_{p1}_{p2}_{p3}_{account_gold}_{curr_time}'\n",
    "        out_file  = os.path.join(out_dir, out_file)\n",
    "                              \n",
    "        with open(out_file+'.json', 'w') as of_json:\n",
    "            params_out = {p: str(params[p]) for p in params.keys()}\n",
    "            json.dump(params_out,fp=of_json, indent=4)\n",
    "\n",
    "        save_to_file(result, out_file+'.tsv', \n",
    "                     ruwordnet)\n",
    "                              \n",
    "        \n",
    "        end = time.time()\n",
    "        logging.info(f\"Saved {out_file}.\")\n",
    "        logging.info(f\"Spent time: {end - start} secs ({(end - start)/60.} minutes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_words = list()\n",
    "private_words = list()\n",
    "\n",
    "def load_words(nouns):\n",
    "    global public_words\n",
    "    global private_words\n",
    "#     if nouns:\n",
    "#         print (ruwordnet.get_synset_senses_list('126551-N'))\n",
    "#     else:\n",
    "#         print (ruwordnet.get_synset_senses_list('124595-V'))\n",
    "\n",
    "#     if nouns:\n",
    "#         print(len([s for s in ruwordnet.synsets_list if s['part_of_speech']=='N']))\n",
    "#     else:\n",
    "#         print(len([s for s in ruwordnet.synsets_list if s['part_of_speech']=='V']))\n",
    "    if nouns:\n",
    "        with open('data/public_test/nouns_public.tsv', 'r') as f:\n",
    "            public_words = [l.strip().lower() for l in f.readlines()]\n",
    "        with open('data/private_test/nouns_private.tsv', 'r') as f:\n",
    "            private_words = [l.strip().lower() for l in f.readlines()]\n",
    "    else:\n",
    "        with open('data/public_test/verbs_public.tsv', 'r') as f:\n",
    "            public_words = [l.strip().lower() for l in f.readlines()]\n",
    "        with open('data/private_test/verbs_private.tsv', 'r') as f:\n",
    "            private_words = [l.strip().lower() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUBLIC NOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = True # False -- Verbs\n",
    "load_words(nouns)\n",
    "for model_name, model in modelname2model.items():\n",
    "    process_words(public_words, \n",
    "                  params_grid, \n",
    "                  '_'.join(('public_nouns',common_prefix, model_name)),\n",
    "                  nouns,\n",
    "                  model=model, \n",
    "                  out_dir=results_dir\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIVATE NOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = True \n",
    "load_words(nouns)\n",
    "for model_name, model in modelname2model.items():\n",
    "    process_words(private_words, \n",
    "                  params_grid, \n",
    "                  '_'.join(('private_nouns',common_prefix, model_name)),\n",
    "                  nouns,\n",
    "                  model=model,\n",
    "                  out_dir=results_dir\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUBLIC VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = False \n",
    "load_words(nouns)\n",
    "for model_name, model in modelname2model.items():\n",
    "    process_words(public_words, \n",
    "                  params_grid, \n",
    "                  '_'.join(('public_verbs',common_prefix, model_name)),\n",
    "                  nouns,\n",
    "                  model=model,\n",
    "                  out_dir=results_dir\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRIVATE VERBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = False \n",
    "load_words(nouns)\n",
    "for model_name, model in modelname2model.items():\n",
    "    process_words(private_words,\n",
    "                  params_grid,\n",
    "                  '_'.join(('private_verbs',common_prefix, model_name)),\n",
    "                  nouns,\n",
    "                  model=model,\n",
    "                  out_dir=results_dir\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_work_dir = '_'.join(('data/elmo_vectors',common_prefix))\n",
    "elmo_model_dir = 'data/rusvectores_models/199'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p {elmo_work_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def prepare_elmo_data(nouns):\n",
    "    if nouns:\n",
    "        make_sentences_params = {'ruthes_name': True,\n",
    "                                 'definition': False,\n",
    "                                 'senses_names': True,\n",
    "                                 'senses_lemmas': False,\n",
    "                                 'senses_main_word': False,\n",
    "                                 'sep': ' '\n",
    "                                }\n",
    "    else:\n",
    "        make_sentences_params = {'ruthes_name': True,\n",
    "                                 'definition': True,\n",
    "                                 'senses_names': True,\n",
    "                                 'senses_lemmas': True,\n",
    "                                 'senses_main_word': True,\n",
    "                                 'sep': ' '\n",
    "                                }\n",
    "    norm_fn_params_list = {'out_of_vocab2synonym',\n",
    "                           'sort',\n",
    "                           'unique',\n",
    "                           'lower',\n",
    "                           'min_word_len',\n",
    "                           'punct_symbols',\n",
    "                           'prepositions',\n",
    "                           'restrict_tags',\n",
    "                           'accept_tags',\n",
    "                           'ma'\n",
    "                           }\n",
    "    norm_fn_params = {param_name: params[param_name] \n",
    "                      for param_name in norm_fn_params_list\n",
    "                      if param_name in params}\n",
    "\n",
    "\n",
    "    synsetid2sentence = dict()\n",
    "    for synset in ruwordnet.synsets_list:\n",
    "        synset_id=synset['id']\n",
    "        if not ((nouns and synset_id.endswith('N')) or \n",
    "                (not nouns and synset_id.endswith('V') )):\n",
    "            continue\n",
    "\n",
    "        words = get_synset_words_lemma(ruwordnet, synset_id,\n",
    "                                       norm_params=norm_fn_params,\n",
    "                                       make_sent_params=make_sentences_params,\n",
    "                                       norm_function=normalize_ma_lemmatize\n",
    "                                      )\n",
    "        sentence = ' '.join(words).strip()\n",
    "        synsetid2sentence[synset_id] = sentence\n",
    "\n",
    "    if nouns:\n",
    "        fname = os.path.join(elmo_work_dir,'sentences_N.txt')\n",
    "        fname_s = os.path.join(elmo_work_dir, 'synsetids_N.txt')\n",
    "    else:\n",
    "        fname = os.path.join(elmo_work_dir,'sentences_V.txt')\n",
    "        fname_s = os.path.join(elmo_work_dir,'synsetids_V.txt')\n",
    "\n",
    "    with open(fname, 'w') as f:\n",
    "        f.writelines([sent+'\\n' for s_id, sent in synsetid2sentence.items()])\n",
    "    with open(fname_s, 'w') as f:\n",
    "        f.writelines([s_id+'\\n' for s_id, sent in synsetid2sentence.items()])\n",
    "\n",
    "prepare_elmo_data(True)\n",
    "prepare_elmo_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nouns = True \n",
    "load_words(nouns)\n",
    "with open(os.path.join(elmo_work_dir,'public_nouns.txt'), 'w') as f:\n",
    "    f.writelines([w.lower()+'\\n' for w in public_words])\n",
    "with open(os.path.join(elmo_work_dir,'private_nouns.txt'), 'w') as f:\n",
    "    f.writelines([w.lower()+'\\n' for w in private_words])\n",
    "nouns = False \n",
    "load_words(nouns)\n",
    "with open(os.path.join(elmo_work_dir,'public_verbs.txt'), 'w') as f:\n",
    "    f.writelines([w.lower()+'\\n' for w in public_words])\n",
    "with open(os.path.join(elmo_work_dir,'private_verbs.txt'), 'w') as f:\n",
    "    f.writelines([w.lower()+'\\n' for w in private_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_work_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get elmo vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd simple_elmo/ && ./make_elmo_vectors_ruwordnet.sh ../{elmo_work_dir} ../{elmo_model_dir} && cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ruwordnet_matrix = np.load(os.path.join(elmo_work_dir,\n",
    "                                        'sentences_N_elmo_avg_vectors_199.npy')\n",
    "                          )\n",
    "ruwordnet_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_nouns_matrix = np.load(os.path.join(elmo_work_dir,\n",
    "                                           'public_nouns_elmo_avg_vectors_199.npy')\n",
    "                             )\n",
    "public_nouns_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(elmo_work_dir,'synsetids_N.txt'), 'r') as f:\n",
    "    synset_ids_N = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -2 {elmo_work_dir}/sentences_N.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 {elmo_work_dir}/public_nouns.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top(public_nouns_matrix[5], ruwordnet_matrix, synset_ids_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top(public_nouns_matrix[0], ruwordnet_matrix, synset_ids_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words_elmo_199(public, \n",
    "                       params_grid,\n",
    "                       prefix, \n",
    "                       nouns,\n",
    "                       elmo_work_dir,\n",
    "                       out_dir='/tmp'):\n",
    "    \n",
    "    for params in tqdm(params_grid()):\n",
    "        start = time.time()\n",
    "        p1,p2,p3,k,topn,account_gold,norm_function = params['p1'],params['p2'],params['p3'],\\\n",
    "                                                     params['k'],params['topn'],params['account_gold'],\\\n",
    "                                                     params['normalize_func']\n",
    "        \n",
    "        pos = \"N\" if nouns else \"V\"\n",
    "        \n",
    "        if nouns:\n",
    "            word2parents = read_train_dataset('data/training_data/synsets_nouns.tsv', ruwordnet)\n",
    "            gold_synsetid2parents = read_gold_dataset('data/training_data/synsets_nouns.tsv')\n",
    "        else:\n",
    "            word2parents = read_train_dataset('data/training_data/synsets_verbs.tsv', ruwordnet)\n",
    "            gold_synsetid2parents = read_gold_dataset('data/training_data/synsets_verbs.tsv')\n",
    "            \n",
    "        if nouns:\n",
    "            ruwordnet_matrix = np.load(os.path.join(elmo_work_dir,'sentences_N_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir, 'synsetids_N.txt'), 'r') as f:\n",
    "                synset_ids = [l.strip() for l in f.readlines()]\n",
    "            with open(os.path.join(elmo_work_dir, 'sentences_N.txt'), 'r') as f:\n",
    "                sentences = [l.strip() for l in f.readlines()]\n",
    "        else:\n",
    "            ruwordnet_matrix = np.load(os.path.join(elmo_work_dir,'sentences_V_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir,'synsetids_V.txt'), 'r') as f:\n",
    "                synset_ids = [l.strip() for l in f.readlines()]\n",
    "            with open(os.path.join(elmo_work_dir,'sentences_V.txt'), 'r') as f:\n",
    "                sentences = [l.strip() for l in f.readlines()]\n",
    "\n",
    "        if nouns and public:\n",
    "            words_matrix = np.load(os.path.join(elmo_work_dir,'public_nouns_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir,'public_nouns.txt'), 'r') as f:\n",
    "                words = [l.strip() for l in f.readlines()]\n",
    "        elif nouns and (not public):\n",
    "            words_matrix = np.load(os.path.join(elmo_work_dir,'private_nouns_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir,'private_nouns.txt'), 'r') as f:\n",
    "                words = [l.strip() for l in f.readlines()]\n",
    "        elif (not nouns) and public:\n",
    "            words_matrix = np.load(os.path.join(elmo_work_dir,'public_verbs_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir,'public_verbs.txt'), 'r') as f:\n",
    "                words = [l.strip() for l in f.readlines()]\n",
    "        elif (not nouns) and (not public):\n",
    "            words_matrix = np.load(os.path.join(elmo_work_dir,'private_verbs_elmo_avg_vectors_199.npy'))\n",
    "            with open(os.path.join(elmo_work_dir,'private_verbs.txt'), 'r') as f:\n",
    "                words = [l.strip() for l in f.readlines()]\n",
    "            \n",
    "        \n",
    "        synsetstr2id = dict()\n",
    "        synsetstr2vector = dict()\n",
    "        \n",
    "        for synsetstr, synsetid, synsetvector in zip(sentences, synset_ids, ruwordnet_matrix):\n",
    "            synsetstr2id[synsetstr] = synsetid\n",
    "            synsetstr2vector[synsetstr] = synsetvector\n",
    "        \n",
    "        result = collections.defaultdict(list)\n",
    "        for w, v in zip(words, words_matrix):\n",
    "            hypernyms = get_top_hyperomyns_counter_v(v, \n",
    "                                                     k=k, \n",
    "                                                     p1=p1,p2=p2,p3=p3, \n",
    "                                                     account_gold=account_gold,\n",
    "                                                     ruwordnet_matrix=ruwordnet_matrix,\n",
    "                                                     gold_synsetid2parents=gold_synsetid2parents,\n",
    "                                                     synsetstr2id=synsetstr2id,\n",
    "                                                     synsetstr2vector=synsetstr2vector,\n",
    "                                                     ruwordnet=ruwordnet\n",
    "                                                    )\n",
    "            result[w] = [h for h,rate in hypernyms.most_common()][:topn]\n",
    "            \n",
    "        \n",
    "        curr_time ='_'.join(str(datetime.now()).split()).replace(':','')\n",
    "        out_file  = f'{prefix}_{k}_{topn}_{p1}_{p2}_{p3}_{account_gold}_{curr_time}'\n",
    "        out_file  = os.path.join(out_dir, out_file)\n",
    "                              \n",
    "        with open(out_file+'.json', 'w') as of_json:\n",
    "            params_out = {p: str(params[p]) for p in params.keys()}\n",
    "            json.dump(params_out,fp=of_json, indent=4)\n",
    "\n",
    "        save_to_file(result, out_file+'.tsv', \n",
    "                     ruwordnet)\n",
    "                              \n",
    "        \n",
    "        end = time.time()\n",
    "        logging.info(f\"Saved {out_file}.\")\n",
    "        logging.info(f\"Spent time: {end - start} secs ({(end - start)/60.} minutes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "public=True\n",
    "nouns=True\n",
    "process_words_elmo_199(public, params_grid, \n",
    "                       '_'.join(('public_nouns_elmo_199',common_prefix)), \n",
    "                       nouns, elmo_work_dir, out_dir=results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "public=True\n",
    "nouns=False\n",
    "process_words_elmo_199(public, params_grid, \n",
    "                       '_'.join(('public_verbs_elmo_199',common_prefix)),\n",
    "                       nouns, elmo_work_dir, out_dir=results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "public=False\n",
    "nouns=True\n",
    "process_words_elmo_199(public, params_grid, \n",
    "                       '_'.join(('private_nouns_elmo_199',common_prefix)),\n",
    "                       nouns, elmo_work_dir, out_dir=results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "public=False\n",
    "nouns=False\n",
    "process_words_elmo_199(public, params_grid, \n",
    "                       '_'.join(('private_verbs_elmo_199',common_prefix)),\n",
    "                       nouns, elmo_work_dir, out_dir=results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
